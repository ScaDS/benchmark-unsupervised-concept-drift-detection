{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Drift Detector Benchmark Evaluation\n",
    "\n",
    "**Purpose**: Unified evaluation notebook for both real-world and synthetic datasets\n",
    "\n",
    "**Modes**:\n",
    "- **Standard Mode**: Real-world datasets with ACCURACY, RUNTIME, REQLABELS metrics\n",
    "- **MTR Mode**: Synthetic datasets with RUNTIME and MTR (Mean Time Ratio) metrics\n",
    "\n",
    "**Last Updated**: 2025-10-01\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "# Shared configuration\n",
    "from eval_config import (\n",
    "    REAL_DATASETS,\n",
    "    SYNTHETIC_DATASETS,\n",
    "    ALL_DETECTORS,\n",
    "    DETECTOR_COLORS,\n",
    "    SINGLE_VARIATE_DETECTORS,\n",
    "    BASE_PATH,\n",
    "    setup_plot_style\n",
    ")\n",
    "\n",
    "# Setup plotting\n",
    "setup_plot_style()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: Choose evaluation mode\n",
    "# ============================================================================\n",
    "\n",
    "# Set to True for MTR evaluation (synthetic datasets), False for standard evaluation\n",
    "MTR_MODE = False  # Change to True for MTR evaluation\n",
    "\n",
    "# Configuration based on mode\n",
    "if MTR_MODE:\n",
    "    # MTR Mode: Synthetic datasets\n",
    "    datasets = SYNTHETIC_DATASETS  # [\"WaveformPre\", \"SineClustersPre\"]\n",
    "    metric1 = \"RUNTIME\"\n",
    "    metric2 = \"MTR\"\n",
    "    metric3 = \"\"\n",
    "    mode_name = \"MTR (Mean Time Ratio)\"\n",
    "else:\n",
    "    # Standard Mode: Real-world datasets\n",
    "    datasets = REAL_DATASETS\n",
    "    metric1 = \"ACCURACY\"\n",
    "    metric2 = \"RUNTIME\"\n",
    "    metric3 = \"\"  # Can add \"REQLABELS\" if needed\n",
    "    mode_name = \"Standard (Accuracy/Runtime)\"\n",
    "\n",
    "# Common configuration\n",
    "detectors = ALL_DETECTORS\n",
    "base_path = BASE_PATH\n",
    "classifier = \"HoeffdingTreeClassifier\"\n",
    "\n",
    "print(f\"=== Evaluation Mode: {mode_name} ===\")\n",
    "print(f\"Datasets: {datasets}\")\n",
    "print(f\"Detectors: {len(detectors)} configured\")\n",
    "print(f\"Metrics: {metric1}, {metric2}\" + (f\", {metric3}\" if metric3 else \"\"))\n",
    "print(f\"Base Path: {base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Experiment Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_detector_dataset(dd, datasets, base_path, metric1, metric2, metric3, mtr_mode=False):\n",
    "    \"\"\"\n",
    "    Process a detector across multiple datasets and collect status information.\n",
    "    \n",
    "    Args:\n",
    "        dd: Drift detector name\n",
    "        datasets: List of dataset names\n",
    "        base_path: Base path to experiment results\n",
    "        metric1, metric2, metric3: Metric names\n",
    "        mtr_mode: If True, use MTR-specific processing\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (detector_name, status_dict)\n",
    "        status_dict: {dataset: [evaluated_count, completed_count]}\n",
    "    \"\"\"\n",
    "    dd_dataset_status = {}\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        dd_dataset_status[dataset] = [0, 0]\n",
    "        \n",
    "        # Construct experiment path\n",
    "        exp_path = f\"{base_path}{dd}_{dataset}_{classifier}_{metric1}-{metric2}\"\n",
    "        if metric3:\n",
    "            exp_path += f\"-{metric3}\"\n",
    "        \n",
    "        if not os.path.exists(exp_path):\n",
    "            print(f\"Experiment not found: {dd}, {dataset}\")\n",
    "            continue\n",
    "        \n",
    "        # Find latest run\n",
    "        try:\n",
    "            nbr_runs = [int(x) for x in os.listdir(exp_path) if x.isdigit()]\n",
    "            if not nbr_runs:\n",
    "                continue\n",
    "            run_path = os.path.join(exp_path, str(max(nbr_runs)), \"results.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding runs for {dd}, {dataset}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(run_path):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Load results\n",
    "            df = pd.read_csv(run_path)\n",
    "            \n",
    "            # Filter out abandoned trials\n",
    "            df = df[df[\"trial_status\"] != \"ABANDONED\"]\n",
    "            df_completed = df[df[\"trial_status\"] == \"COMPLETED\"]\n",
    "            \n",
    "            evaluated = len(df)\n",
    "            completed = len(df_completed)\n",
    "            \n",
    "            dd_dataset_status[dataset][0] = evaluated\n",
    "            dd_dataset_status[dataset][1] = completed\n",
    "            \n",
    "            # Check for timeout jobs (if many evaluations but few completions)\n",
    "            if evaluated > 500 and completed < 0.1 * evaluated and completed < 500:\n",
    "                # Analyze timeout patterns\n",
    "                not_in_timeout = 0\n",
    "                total_checked = 0\n",
    "                \n",
    "                nbr_runs_sorted = sorted(nbr_runs, reverse=True)\n",
    "                \n",
    "                for nbr_run in nbr_runs_sorted[:5]:  # Check last 5 runs\n",
    "                    single_runs_path = os.path.join(\n",
    "                        exp_path, str(nbr_run), \"single_runs\"\n",
    "                    )\n",
    "                    \n",
    "                    if os.path.isdir(single_runs_path):\n",
    "                        single_run_jobs = [\n",
    "                            name for name in os.listdir(single_runs_path)\n",
    "                            if os.path.isdir(os.path.join(single_runs_path, name))\n",
    "                        ]\n",
    "                        \n",
    "                        for job in single_run_jobs[:50]:  # Check up to 50 jobs\n",
    "                            log_file = os.path.join(\n",
    "                                single_runs_path, job, f\"{job}_0_log.out\"\n",
    "                            )\n",
    "                            \n",
    "                            if os.path.exists(log_file):\n",
    "                                total_checked += 1\n",
    "                                with open(log_file, 'r') as f:\n",
    "                                    log_content = f.read()\n",
    "                                    # Count non-timeout jobs\n",
    "                                    if \"DUE TO TIME LIMIT\" not in log_content or \\\n",
    "                                       (\"DUE TO TIME LIMIT\" in log_content and \"oom_kill\" in log_content):\n",
    "                                        not_in_timeout += 1\n",
    "                        \n",
    "                        if total_checked >= 200:\n",
    "                            break\n",
    "                \n",
    "                # If >80% are timeouts, flag it\n",
    "                if total_checked > 0 and not_in_timeout / total_checked < 0.2:\n",
    "                    print(f\"⚠️  Timeout job detected: {dd} on {dataset} \"\n",
    "                          f\"({not_in_timeout}/{total_checked} non-timeout)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dd} on {dataset}: {e}\")\n",
    "    \n",
    "    return dd, dd_dataset_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "count-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count experiments using parallel processing\n",
    "print(f\"\\n=== Counting Experiments ===\")\n",
    "print(f\"Using {multiprocessing.cpu_count()} CPUs for parallel processing\\n\")\n",
    "\n",
    "with Pool(multiprocessing.cpu_count()) as pool:\n",
    "    results = pool.starmap(\n",
    "        process_detector_dataset,\n",
    "        [(dd, datasets, base_path, metric1, metric2, metric3, MTR_MODE) \n",
    "         for dd in detectors]\n",
    "    )\n",
    "\n",
    "# Organize results\n",
    "detector_status = {dd: status for dd, status in results}\n",
    "\n",
    "print(f\"\\n=== Experiment Status Summary ===\")\n",
    "print(detector_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 3. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "total_combinations = len(detectors) * len(datasets)\n",
    "total_evaluated = 0\n",
    "total_completed = 0\n",
    "\n",
    "for dd, status in detector_status.items():\n",
    "    for dataset, counts in status.items():\n",
    "        if counts[0] > 0:  # Has evaluations\n",
    "            total_evaluated += 1\n",
    "        if counts[1] >= 500:  # Has sufficient completions\n",
    "            total_completed += 1\n",
    "\n",
    "print(f\"\\n=== Overall Statistics ===\")\n",
    "print(f\"Total detector-dataset combinations: {total_combinations}\")\n",
    "print(f\"Combinations with evaluations: {total_evaluated} ({100*total_evaluated/total_combinations:.1f}%)\")\n",
    "print(f\"Combinations with ≥500 completions: {total_completed} ({100*total_completed/total_combinations:.1f}%)\")\n",
    "\n",
    "# Per-detector summary\n",
    "print(f\"\\n=== Per-Detector Summary ===\")\n",
    "detector_summary = []\n",
    "\n",
    "for dd in detectors:\n",
    "    status = detector_status.get(dd, {})\n",
    "    datasets_evaluated = sum(1 for counts in status.values() if counts[0] > 0)\n",
    "    datasets_completed = sum(1 for counts in status.values() if counts[1] >= 500)\n",
    "    \n",
    "    detector_summary.append({\n",
    "        'Detector': dd,\n",
    "        'Datasets Evaluated': datasets_evaluated,\n",
    "        'Datasets Completed': datasets_completed,\n",
    "        'Completion Rate': f\"{100*datasets_completed/len(datasets):.1f}%\" if len(datasets) > 0 else \"0%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(detector_summary)\n",
    "summary_df = summary_df.sort_values('Datasets Completed', ascending=False)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-header",
   "metadata": {},
   "source": [
    "## 4. Visualization: Experiment Completion Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completion-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of completion status\n",
    "completion_matrix = np.zeros((len(detectors), len(datasets)))\n",
    "\n",
    "for i, dd in enumerate(detectors):\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        status = detector_status.get(dd, {}).get(dataset, [0, 0])\n",
    "        completion_matrix[i, j] = status[1]  # Completed count\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    completion_matrix,\n",
    "    xticklabels=datasets,\n",
    "    yticklabels=detectors,\n",
    "    annot=True,\n",
    "    fmt='.0f',\n",
    "    cmap='YlGnBu',\n",
    "    cbar_kws={'label': 'Completed Experiments'},\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title(f'Experiment Completion Status - {mode_name}', fontsize=14, pad=20)\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Drift Detector', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-analysis-header",
   "metadata": {},
   "source": [
    "## 5. Threshold Analysis\n",
    "\n",
    "Analyze how many detector-dataset combinations meet different completion thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threshold-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze completion at different thresholds\n",
    "thresholds = list(range(0, 501, 50))\n",
    "thresholds[0] = 1  # Replace 0 with 1\n",
    "threshold_counts = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    count = 0\n",
    "    for dd in detectors:\n",
    "        for dataset in datasets:\n",
    "            status = detector_status.get(dd, {}).get(dataset, [0, 0])\n",
    "            if status[1] >= threshold:  # Completed count >= threshold\n",
    "                count += 1\n",
    "    threshold_counts.append(count)\n",
    "\n",
    "# Plot threshold analysis\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(range(len(thresholds)), threshold_counts, color='steelblue', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, count) in enumerate(zip(bars, threshold_counts)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count}',\n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('Minimum Completed Experiments', fontsize=12)\n",
    "ax.set_ylabel('Number of Detector-Dataset Combinations', fontsize=12)\n",
    "ax.set_title(f'Completion Threshold Analysis - {mode_name}', fontsize=14, pad=20)\n",
    "ax.set_xticks(range(len(thresholds)))\n",
    "ax.set_xticklabels([str(t) for t in thresholds], rotation=45)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Threshold Analysis ===\")\n",
    "for threshold, count in zip(thresholds, threshold_counts):\n",
    "    print(f\"≥{threshold:3d} completions: {count:3d}/{total_combinations} \"\n",
    "          f\"({100*count/total_combinations:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-status-header",
   "metadata": {},
   "source": [
    "## 6. Detailed Status by Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show detailed status for each dataset\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    dataset_data = []\n",
    "    for dd in detectors:\n",
    "        status = detector_status.get(dd, {}).get(dataset, [0, 0])\n",
    "        evaluated, completed = status\n",
    "        \n",
    "        if evaluated > 0:\n",
    "            completion_rate = 100 * completed / evaluated\n",
    "            dataset_data.append({\n",
    "                'Detector': dd,\n",
    "                'Evaluated': evaluated,\n",
    "                'Completed': completed,\n",
    "                'Rate': f\"{completion_rate:.1f}%\"\n",
    "            })\n",
    "    \n",
    "    if dataset_data:\n",
    "        dataset_df = pd.DataFrame(dataset_data)\n",
    "        dataset_df = dataset_df.sort_values('Completed', ascending=False)\n",
    "        print(dataset_df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"No experiments found for this dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-experiments-header",
   "metadata": {},
   "source": [
    "## 7. Identify Missing or Incomplete Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find experiments that need attention\n",
    "missing_experiments = []\n",
    "incomplete_experiments = []\n",
    "\n",
    "for dd in detectors:\n",
    "    for dataset in datasets:\n",
    "        status = detector_status.get(dd, {}).get(dataset, [0, 0])\n",
    "        evaluated, completed = status\n",
    "        \n",
    "        if evaluated == 0:\n",
    "            missing_experiments.append((dd, dataset))\n",
    "        elif completed < 500 and evaluated > 0:\n",
    "            incomplete_experiments.append((dd, dataset, evaluated, completed))\n",
    "\n",
    "print(f\"\\n=== Missing Experiments (Not Started) ===\")\n",
    "print(f\"Total: {len(missing_experiments)}\")\n",
    "if missing_experiments:\n",
    "    for dd, dataset in missing_experiments[:20]:  # Show first 20\n",
    "        print(f\"  - {dd} on {dataset}\")\n",
    "    if len(missing_experiments) > 20:\n",
    "        print(f\"  ... and {len(missing_experiments) - 20} more\")\n",
    "else:\n",
    "    print(\"  None! All experiments have been started.\")\n",
    "\n",
    "print(f\"\\n=== Incomplete Experiments (<500 completions) ===\")\n",
    "print(f\"Total: {len(incomplete_experiments)}\")\n",
    "if incomplete_experiments:\n",
    "    incomplete_df = pd.DataFrame(\n",
    "        incomplete_experiments,\n",
    "        columns=['Detector', 'Dataset', 'Evaluated', 'Completed']\n",
    "    )\n",
    "    incomplete_df = incomplete_df.sort_values('Completed', ascending=True)\n",
    "    print(incomplete_df.head(20).to_string(index=False))\n",
    "    if len(incomplete_experiments) > 20:\n",
    "        print(f\"\\n  ... and {len(incomplete_experiments) - 20} more\")\n",
    "else:\n",
    "    print(\"  None! All started experiments have ≥500 completions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions-header",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"EVALUATION SUMMARY - {mode_name}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Mode: {mode_name}\")\n",
    "print(f\"  - Datasets: {len(datasets)} ({', '.join(datasets[:3])}...)\")\n",
    "print(f\"  - Detectors: {len(detectors)}\")\n",
    "print(f\"  - Metrics: {metric1}, {metric2}\" + (f\", {metric3}\" if metric3 else \"\"))\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  - Total combinations: {total_combinations}\")\n",
    "print(f\"  - With evaluations: {total_evaluated} ({100*total_evaluated/total_combinations:.1f}%)\")\n",
    "print(f\"  - With ≥500 completions: {total_completed} ({100*total_completed/total_combinations:.1f}%)\")\n",
    "print(f\"  - Missing (not started): {len(missing_experiments)}\")\n",
    "print(f\"  - Incomplete (<500): {len(incomplete_experiments)}\")\n",
    "print(f\"\\nNext Steps:\")\n",
    "if missing_experiments:\n",
    "    print(f\"  1. Start {len(missing_experiments)} missing experiments\")\n",
    "if incomplete_experiments:\n",
    "    print(f\"  2. Continue {len(incomplete_experiments)} incomplete experiments\")\n",
    "if not missing_experiments and not incomplete_experiments:\n",
    "    print(f\"  ✓ All experiments complete! Ready for analysis.\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chathpc@alpha",
   "language": "python",
   "name": "chathpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
