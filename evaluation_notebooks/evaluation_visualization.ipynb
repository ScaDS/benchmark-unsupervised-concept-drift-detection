{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Drift Detector Performance Visualization\n",
    "\n",
    "**Purpose**: Comprehensive visualization of drift detector performance across datasets\n",
    "\n",
    "**Visualizations**:\n",
    "- Pareto fronts for multi-objective optimization\n",
    "- Scatter plots showing accuracy vs runtime trade-offs\n",
    "- MTR vs Runtime plots for synthetic datasets\n",
    "- Baseline comparisons\n",
    "- Performance trends across different configurations\n",
    "\n",
    "**Last Updated**: 2025-10-01\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from statistics import mean, stdev, median\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Shared configuration\n",
    "from eval_config import (\n",
    "    REAL_DATASETS,\n",
    "    SYNTHETIC_DATASETS,\n",
    "    ALL_DETECTORS,\n",
    "    DETECTOR_COLORS,\n",
    "    BASE_PATH,\n",
    "    setup_plot_style\n",
    ")\n",
    "\n",
    "# Setup plotting\n",
    "setup_plot_style()\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Choose evaluation mode\n",
    "MTR_MODE = False  # Set to True for MTR evaluation (synthetic datasets)\n",
    "\n",
    "# Configuration based on mode\n",
    "if MTR_MODE:\n",
    "    datasets = SYNTHETIC_DATASETS\n",
    "    metric1 = \"RUNTIME\"\n",
    "    metric2 = \"MTR\"\n",
    "    mode_name = \"MTR\"\n",
    "else:\n",
    "    datasets = REAL_DATASETS\n",
    "    metric1 = \"ACCURACY\"\n",
    "    metric2 = \"RUNTIME\"\n",
    "    mode_name = \"Standard\"\n",
    "\n",
    "detectors = ALL_DETECTORS\n",
    "colors = DETECTOR_COLORS\n",
    "base_path = BASE_PATH\n",
    "classifier = \"HoeffdingTreeClassifier\"\n",
    "\n",
    "print(f\"Mode: {mode_name}\")\n",
    "print(f\"Datasets: {datasets}\")\n",
    "print(f\"Metrics: {metric1} vs {metric2}\")\n",
    "print(f\"Detectors: {len(detectors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-functions-header",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pareto_efficient_mixed(points, maximize):\n",
    "    \"\"\"\n",
    "    Find Pareto-efficient points when some objectives are to be maximized and others minimized.\n",
    "    \n",
    "    Args:\n",
    "        points: numpy array of shape (n_points, n_objectives)\n",
    "        maximize: list/array of bools, True if objective is to be maximized, else minimized\n",
    "    \n",
    "    Returns:\n",
    "        Boolean array indicating whether each point is Pareto efficient\n",
    "    \"\"\"\n",
    "    points = np.asarray(points)\n",
    "    points = points.astype(float)\n",
    "    maximize = np.asarray(maximize)\n",
    "    \n",
    "    if maximize.shape[0] != points.shape[1]:\n",
    "        raise ValueError(\"Length of 'maximize' must match number of objectives.\")\n",
    "    \n",
    "    # Convert maximization objectives to minimization by negating them\n",
    "    adjusted_points = points.copy()\n",
    "    for i, to_maximize in enumerate(maximize):\n",
    "        if to_maximize:\n",
    "            adjusted_points[:, i] = -adjusted_points[:, i]\n",
    "    \n",
    "    n_points = adjusted_points.shape[0]\n",
    "    is_efficient = np.ones(n_points, dtype=bool)\n",
    "    \n",
    "    for i in range(n_points):\n",
    "        if not is_efficient[i]:\n",
    "            continue\n",
    "        # A point is dominated if another point is better in all objectives\n",
    "        dominates = np.all(adjusted_points <= adjusted_points[i], axis=1) & \\\n",
    "                   np.any(adjusted_points < adjusted_points[i], axis=1)\n",
    "        is_efficient[i] = not np.any(dominates)\n",
    "        if is_efficient[i]:\n",
    "            dominated_by_i = np.all(adjusted_points[i] <= adjusted_points, axis=1) & \\\n",
    "                            np.any(adjusted_points[i] < adjusted_points, axis=1)\n",
    "            is_efficient[dominated_by_i] = False\n",
    "            is_efficient[i] = True\n",
    "    \n",
    "    return is_efficient\n",
    "\n",
    "\n",
    "def pareto_front_simple(x, y):\n",
    "    \"\"\"\n",
    "    Simple Pareto front calculation for 2D case (maximize x, minimize y).\n",
    "    \n",
    "    Args:\n",
    "        x: Array of first objective values\n",
    "        y: Array of second objective values\n",
    "    \n",
    "    Returns:\n",
    "        Indices of Pareto-efficient points\n",
    "    \"\"\"\n",
    "    is_dominated = np.zeros(len(x), dtype=bool)\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x)):\n",
    "            if (x[j] >= x[i] and y[j] <= y[i]) and (x[j] != x[i] or y[j] != y[i]):\n",
    "                is_dominated[i] = True\n",
    "                break\n",
    "    return np.where(~is_dominated)[0]\n",
    "\n",
    "\n",
    "def load_experiment_results(detector, dataset, metric1, metric2, base_path=BASE_PATH):\n",
    "    \"\"\"\n",
    "    Load experiment results for a detector-dataset combination.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results or None if not found\n",
    "    \"\"\"\n",
    "    exp_path = f\"{base_path}{detector}_{dataset}_{classifier}_{metric1}-{metric2}\"\n",
    "    \n",
    "    if not os.path.exists(exp_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        nbr_runs = [d for d in os.listdir(exp_path) if d.isdigit()]\n",
    "        if not nbr_runs:\n",
    "            return None\n",
    "        \n",
    "        run_path = os.path.join(exp_path, max(nbr_runs), \"results.csv\")\n",
    "        if not os.path.exists(run_path):\n",
    "            return None\n",
    "        \n",
    "        df = pd.read_csv(run_path)\n",
    "        df = df[[metric1, metric2]].dropna()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {detector} on {dataset}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pareto-header",
   "metadata": {},
   "source": [
    "## 3. Pareto Front Visualization\n",
    "\n",
    "Visualize Pareto-optimal configurations for each detector on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pareto-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pareto fronts for all detectors on each dataset\n",
    "plt.close('all')\n",
    "\n",
    "for dataset in datasets:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    pareto_data = {}\n",
    "    \n",
    "    for dd in detectors:\n",
    "        df = load_experiment_results(dd, dataset, metric1, metric2, base_path)\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            x = df[metric1].values\n",
    "            y = df[metric2].values\n",
    "            \n",
    "            # Calculate Pareto front\n",
    "            if MTR_MODE:\n",
    "                # MTR mode: maximize MTR, minimize RUNTIME\n",
    "                objectives = np.vstack((x, y)).T\n",
    "                maximize = [True, False]  # [MTR, RUNTIME]\n",
    "                pareto_mask = is_pareto_efficient_mixed(objectives, maximize)\n",
    "                pareto_points = objectives[pareto_mask]\n",
    "                \n",
    "                # Sort by MTR for plotting\n",
    "                sorted_indices = np.argsort(pareto_points[:, 0])\n",
    "                pareto_x = pareto_points[sorted_indices, 0]\n",
    "                pareto_y = pareto_points[sorted_indices, 1]\n",
    "            else:\n",
    "                # Standard mode: maximize ACCURACY, minimize RUNTIME\n",
    "                pareto_indices = pareto_front_simple(x, y)\n",
    "                pareto_x = x[pareto_indices]\n",
    "                pareto_y = y[pareto_indices]\n",
    "                \n",
    "                # Sort by accuracy for plotting\n",
    "                sorted_indices = np.argsort(pareto_x)\n",
    "                pareto_x = pareto_x[sorted_indices]\n",
    "                pareto_y = pareto_y[sorted_indices]\n",
    "            \n",
    "            # Plot Pareto front\n",
    "            ax.scatter(pareto_x, pareto_y, \n",
    "                      color=colors.get(dd, '#808080'),\n",
    "                      label=dd,\n",
    "                      alpha=0.8,\n",
    "                      s=60,\n",
    "                      edgecolors='black',\n",
    "                      linewidth=0.5)\n",
    "            \n",
    "            pareto_data[dd] = (pareto_x, pareto_y)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel(metric1, fontsize=12)\n",
    "    ax.set_ylabel(metric2, fontsize=12)\n",
    "    ax.set_title(f'Pareto Fronts: {dataset} ({mode_name} Mode)', fontsize=14, pad=15)\n",
    "    \n",
    "    # Legend\n",
    "    legend = ax.legend(ncol=3, fontsize=9, loc='best')\n",
    "    for handle in legend.legend_handles:\n",
    "        handle.set_sizes([50.0])\n",
    "        handle.set_alpha(1.0)\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPareto front sizes for {dataset}:\")\n",
    "    for dd, (px, py) in pareto_data.items():\n",
    "        print(f\"  {dd}: {len(px)} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scatter-header",
   "metadata": {},
   "source": [
    "## 4. Full Scatter Plots\n",
    "\n",
    "Show all evaluated configurations (not just Pareto front) for selected detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific detectors to visualize in detail\n",
    "selected_detectors = [\"CSDDM\", \"BNDM\", \"D3\", \"IBDD\", \"OCDD\", \"SPLL\"]\n",
    "\n",
    "for dataset in datasets[:3]:  # Show first 3 datasets\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for dd in selected_detectors:\n",
    "        df = load_experiment_results(dd, dataset, metric1, metric2, base_path)\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            x = df[metric1].values\n",
    "            y = df[metric2].values\n",
    "            \n",
    "            # Plot all points\n",
    "            ax.scatter(x, y,\n",
    "                      color=colors.get(dd, '#808080'),\n",
    "                      label=dd,\n",
    "                      alpha=0.5,\n",
    "                      s=30)\n",
    "    \n",
    "    ax.set_xlabel(metric1, fontsize=12)\n",
    "    ax.set_ylabel(metric2, fontsize=12)\n",
    "    ax.set_title(f'All Configurations: {dataset}', fontsize=14, pad=15)\n",
    "    ax.legend(ncol=2, fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-detector-header",
   "metadata": {},
   "source": [
    "## 5. Single Detector Analysis\n",
    "\n",
    "Detailed view of a single detector's performance across configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a detector to analyze in detail\n",
    "target_detector = \"CSDDM\"  # Change this to analyze different detectors\n",
    "target_dataset = datasets[0]  # First dataset\n",
    "\n",
    "df = load_experiment_results(target_detector, target_dataset, metric1, metric2, base_path)\n",
    "\n",
    "if df is not None and len(df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = df[metric1].values\n",
    "    y = df[metric2].values\n",
    "    \n",
    "    # Scatter plot\n",
    "    scatter = ax.scatter(x, y, c=range(len(x)), cmap='viridis', alpha=0.6, s=50)\n",
    "    \n",
    "    # Colorbar showing evaluation order\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Evaluation Order', rotation=270, labelpad=20)\n",
    "    \n",
    "    ax.set_xlabel(metric1, fontsize=12)\n",
    "    ax.set_ylabel(metric2, fontsize=12)\n",
    "    ax.set_title(f'{target_detector} on {target_dataset}\\n{len(df)} Configurations Evaluated',\n",
    "                fontsize=14, pad=15)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\n{target_detector} on {target_dataset} Statistics:\")\n",
    "    print(f\"  {metric1}: {x.min():.3f} - {x.max():.3f} (mean: {x.mean():.3f})\")\n",
    "    print(f\"  {metric2}: {y.min():.1f} - {y.max():.1f} (mean: {y.mean():.1f})\")\n",
    "else:\n",
    "    print(f\"No data found for {target_detector} on {target_dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## 6. Best Configuration Comparison\n",
    "\n",
    "Compare the best configuration of each detector across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect best results for each detector on each dataset\n",
    "best_results = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    best_results[dataset] = {}\n",
    "    \n",
    "    for dd in detectors:\n",
    "        df = load_experiment_results(dd, dataset, metric1, metric2, base_path)\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            # Find best configuration (highest metric1, lowest metric2)\n",
    "            if MTR_MODE:\n",
    "                # For MTR: maximize MTR, minimize RUNTIME\n",
    "                # Use simple scoring: normalize and combine\n",
    "                score = (df[metric1] - df[metric1].min()) / (df[metric1].max() - df[metric1].min() + 1e-10) - \\\n",
    "                       (df[metric2] - df[metric2].min()) / (df[metric2].max() - df[metric2].min() + 1e-10)\n",
    "            else:\n",
    "                # For standard: maximize ACCURACY, minimize RUNTIME\n",
    "                score = (df[metric1] - df[metric1].min()) / (df[metric1].max() - df[metric1].min() + 1e-10) - \\\n",
    "                       (df[metric2] - df[metric2].min()) / (df[metric2].max() - df[metric2].min() + 1e-10)\n",
    "            \n",
    "            best_idx = score.argmax()\n",
    "            best_results[dataset][dd] = {\n",
    "                metric1: df[metric1].iloc[best_idx],\n",
    "                metric2: df[metric2].iloc[best_idx]\n",
    "            }\n",
    "\n",
    "# Plot comparison\n",
    "for dataset in datasets:\n",
    "    if not best_results[dataset]:\n",
    "        continue\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    dds_with_results = list(best_results[dataset].keys())\n",
    "    x_vals = [best_results[dataset][dd][metric1] for dd in dds_with_results]\n",
    "    y_vals = [best_results[dataset][dd][metric2] for dd in dds_with_results]\n",
    "    colors_list = [colors.get(dd, '#808080') for dd in dds_with_results]\n",
    "    \n",
    "    ax.scatter(x_vals, y_vals, c=colors_list, s=100, alpha=0.7, edgecolors='black', linewidth=1)\n",
    "    \n",
    "    # Add labels\n",
    "    for dd, x, y in zip(dds_with_results, x_vals, y_vals):\n",
    "        ax.annotate(dd, (x, y), fontsize=8, alpha=0.7,\n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax.set_xlabel(metric1, fontsize=12)\n",
    "    ax.set_ylabel(metric2, fontsize=12)\n",
    "    ax.set_title(f'Best Configurations: {dataset}', fontsize=14, pad=15)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap-header",
   "metadata": {},
   "source": [
    "## 7. Performance Heatmap\n",
    "\n",
    "Heatmap showing best metric1 value for each detector-dataset combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance matrix\n",
    "performance_matrix = np.zeros((len(detectors), len(datasets)))\n",
    "\n",
    "for i, dd in enumerate(detectors):\n",
    "    for j, dataset in enumerate(datasets):\n",
    "        if dataset in best_results and dd in best_results[dataset]:\n",
    "            performance_matrix[i, j] = best_results[dataset][dd][metric1]\n",
    "        else:\n",
    "            performance_matrix[i, j] = np.nan\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(performance_matrix,\n",
    "           xticklabels=datasets,\n",
    "           yticklabels=detectors,\n",
    "           annot=True,\n",
    "           fmt='.2f',\n",
    "           cmap='RdYlGn',\n",
    "           cbar_kws={'label': f'Best {metric1}'},\n",
    "           ax=ax)\n",
    "\n",
    "ax.set_title(f'Best {metric1} by Detector and Dataset', fontsize=14, pad=20)\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Drift Detector', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "summary_data = []\n",
    "\n",
    "for dd in detectors:\n",
    "    metric1_values = []\n",
    "    metric2_values = []\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        if dataset in best_results and dd in best_results[dataset]:\n",
    "            metric1_values.append(best_results[dataset][dd][metric1])\n",
    "            metric2_values.append(best_results[dataset][dd][metric2])\n",
    "    \n",
    "    if metric1_values:\n",
    "        summary_data.append({\n",
    "            'Detector': dd,\n",
    "            f'{metric1} Mean': np.mean(metric1_values),\n",
    "            f'{metric1} Std': np.std(metric1_values),\n",
    "            f'{metric2} Mean': np.mean(metric2_values),\n",
    "            f'{metric2} Std': np.std(metric2_values),\n",
    "            'Datasets': len(metric1_values)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values(f'{metric1} Mean', ascending=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY: Best Configuration Performance ({mode_name} Mode)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(summary_df.to_string(index=False, float_format='%.3f'))\n",
    "print(f\"\\nNote: Statistics computed across {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Save best configurations to CSV for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten best_results for export\n",
    "export_data = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dd in detectors:\n",
    "        if dataset in best_results and dd in best_results[dataset]:\n",
    "            export_data.append({\n",
    "                'Dataset': dataset,\n",
    "                'Detector': dd,\n",
    "                metric1: best_results[dataset][dd][metric1],\n",
    "                metric2: best_results[dataset][dd][metric2]\n",
    "            })\n",
    "\n",
    "export_df = pd.DataFrame(export_data)\n",
    "output_file = f'best_configurations_{mode_name.lower()}.csv'\n",
    "export_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nBest configurations exported to: {output_file}\")\n",
    "print(f\"Total configurations: {len(export_df)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(export_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ab972-0758-4e1f-a446-30bea170b7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chathpc@alpha",
   "language": "python",
   "name": "chathpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
